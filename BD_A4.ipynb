{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSOUcpCEtcK5"
   },
   "source": [
    "#### BDP Assignment 4\n",
    "\n",
    "#### ID: 202318002\n",
    "#### Name: Bhavik Manwani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZUJr__fVfM1",
    "outputId": "a31b80f8-70d6-45a5-b94d-d03b6f8fbb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=7456ac03b01f8f73e79388cb375abeca00f0c7603bb29c3f46e839ab9748b2f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "# installing pyspark\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lJ_BMCSWVi8l"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "D0t7muBMXEwq",
    "outputId": "8dbf7a73-2fc6-46b8-f12c-d9322379bc31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4b970f248bb3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f516051e170>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating spark session using app name and local master\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDD\").master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me6fASMBuHs1"
   },
   "source": [
    "##### RDD Task 1\n",
    "Generate 100 random numbers in range 0 to 10 using numpy randint function with the seed set to 10. Create a RDD using the parallelize function using data generated in previous step. Calculate the frequency of each number (0 - 10) using appropriate function of RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oa7uh77FlrRb",
    "outputId": "1db33c57-a1fa-484e-8f70-29337c0eb796"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 14),\n",
       " (1, 14),\n",
       " (2, 9),\n",
       " (3, 6),\n",
       " (4, 8),\n",
       " (5, 5),\n",
       " (6, 11),\n",
       " (7, 5),\n",
       " (8, 14),\n",
       " (9, 14)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# generating random array from 0 to 10 of size 100\n",
    "arr = np.random.randint(0, 10, 100)\n",
    "\n",
    "# parellelizing arr using spark context parellelize\n",
    "num_rdd = spark.sparkContext.parallelize(arr)\n",
    "\n",
    "# mapping each number to frequency 1\n",
    "num_rdd = num_rdd.map(lambda x: (x,1))\n",
    "\n",
    "# counting frequency of each number\n",
    "num_count = num_rdd.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# sorting numbers by value\n",
    "num_count.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHeGRLHTudjo"
   },
   "source": [
    "##### RDD Task 2\n",
    "In this task you will calculate the frequency of each word in text8 dataset mentioned above. Create a RDD using the text8 dataset.\n",
    "Use appropriate functions of the RDD to get the word frequencies. Filter the RDD using appropriate function to get the frequencies of words containing the letter ’a’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffJYa6MLXF-O",
    "outputId": "b68aa783-550b-4756-9b57-ff9dab4b0687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text8 MapPartitionsRDD[186] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading text file through spark context\n",
    "\n",
    "rdd = spark.sparkContext.textFile('text8') # RDD created\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9wjYzaBXrPN",
    "outputId": "1ad754cc-08d0-4a9a-dd9e-dad9eac2b528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'abuse',\n",
       " 'against',\n",
       " 'early',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'and']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# splitting file by whitespaces to fetch words by rdd flatMap\n",
    "rdd_split = rdd.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# filtering rdd object for words containing 'a'\n",
    "rdd_filtered = rdd_split.filter(lambda x : 'a' in x)\n",
    "\n",
    "rdd_filtered.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Migy215hYVtq",
    "outputId": "e7ec20b0-fbcc-41ed-f33f-a86a8f548c45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 325873),\n",
       " ('aa', 276),\n",
       " ('aaa', 57),\n",
       " ('aaaa', 7),\n",
       " ('aaaaaacceglllnorst', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# assigning each word single frequency\n",
    "rdd_filtered = rdd_filtered.map(lambda x: (x,1))\n",
    "\n",
    "# counting frequency of each word\n",
    "word_counts = rdd_filtered.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# sortign words by value\n",
    "word_counts = word_counts.sortByKey()\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y8DT3uNuq-e"
   },
   "source": [
    "##### DataFrame Task:\n",
    "Create a Spark dataframe using the iris json data mentioned above. Calculate Pearson Correlation between the columns petalLength and petalWidth using the appropriate dataframe API.\n",
    "Show the columns sepalLength, sepalWidth and species for the rows of data that has petalLength greater than or equal to 1.4 using the appropriate dataframe API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOHgNpl4j7m4",
    "outputId": "2b8e0ec1-d803-46ed-a2b3-89a4167b4b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       NULL|      NULL|       NULL|      NULL|   NULL|\n",
      "|           NULL|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|           NULL|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|           NULL|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|           NULL|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|           NULL|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "|           NULL|        1.7|       0.4|        5.4|       3.9| setosa|\n",
      "|           NULL|        1.4|       0.3|        4.6|       3.4| setosa|\n",
      "|           NULL|        1.5|       0.2|        5.0|       3.4| setosa|\n",
      "|           NULL|        1.4|       0.2|        4.4|       2.9| setosa|\n",
      "|           NULL|        1.5|       0.1|        4.9|       3.1| setosa|\n",
      "|           NULL|        1.5|       0.2|        5.4|       3.7| setosa|\n",
      "|           NULL|        1.6|       0.2|        4.8|       3.4| setosa|\n",
      "|           NULL|        1.4|       0.1|        4.8|       3.0| setosa|\n",
      "|           NULL|        1.1|       0.1|        4.3|       3.0| setosa|\n",
      "|           NULL|        1.2|       0.2|        5.8|       4.0| setosa|\n",
      "|           NULL|        1.5|       0.4|        5.7|       4.4| setosa|\n",
      "|           NULL|        1.3|       0.4|        5.4|       3.9| setosa|\n",
      "|           NULL|        1.4|       0.3|        5.1|       3.5| setosa|\n",
      "|           NULL|        1.7|       0.3|        5.7|       3.8| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reading json file\n",
    "df = spark.read.json(\"iris.json\")\n",
    "df.show()\n",
    "\n",
    "# Here df shows '_corrupt_record' column which is to be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3VNr5rJrsAj",
    "outputId": "9662262c-06e6-42fd-95ac-f8f79802eebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation: 0.9626417223780231 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# discarding column with index 1 from df\n",
    "df1 = df.select(df.columns[1:])\n",
    "\n",
    "# calculating pearson correlation between columns\n",
    "c = df1.corr('petalLength', 'petalWidth', method='pearson')\n",
    "\n",
    "print(f'\\nCorrelation: {c} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fijOBb-Tu-N5",
    "outputId": "65a4cd60-600a-4749-b4a5-f26163211284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "|        1.7|       0.4|        5.4|       3.9| setosa|\n",
      "|        1.4|       0.3|        4.6|       3.4| setosa|\n",
      "|        1.5|       0.2|        5.0|       3.4| setosa|\n",
      "|        1.4|       0.2|        4.4|       2.9| setosa|\n",
      "|        1.5|       0.1|        4.9|       3.1| setosa|\n",
      "|        1.5|       0.2|        5.4|       3.7| setosa|\n",
      "|        1.6|       0.2|        4.8|       3.4| setosa|\n",
      "|        1.4|       0.1|        4.8|       3.0| setosa|\n",
      "|        1.5|       0.4|        5.7|       4.4| setosa|\n",
      "|        1.4|       0.3|        5.1|       3.5| setosa|\n",
      "|        1.7|       0.3|        5.7|       3.8| setosa|\n",
      "|        1.5|       0.3|        5.1|       3.8| setosa|\n",
      "|        1.7|       0.2|        5.4|       3.4| setosa|\n",
      "|        1.5|       0.4|        5.1|       3.7| setosa|\n",
      "|        1.7|       0.5|        5.1|       3.3| setosa|\n",
      "|        1.9|       0.2|        4.8|       3.4| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering records with given condition\n",
    "df1.filter((df1.petalLength >= 1.4)).show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
